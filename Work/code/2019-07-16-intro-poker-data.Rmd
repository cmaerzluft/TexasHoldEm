---
title: Texas Holdem. Intro to Data.
author: Christopher Maerzluft
date: '2019-07-16'
slug: intro-poker-data
categories: []
tags: ['Texas HoldEm', 'Introduction']
header:
  caption: ''
  image: ''
---

```{r setup, echo = FALSE}
library(knitr)
library(kableExtra)
```

## Poker! I Barely Know her!

I recently listened to a story on the podcast for [Moth Radio Hour](https://themoth.org/stories/the-big-things-you-dont-do). The speaker, [Annie Duke](https://en.wikipedia.org/wiki/Annie_Duke), described how she came to be in a high stakes game of Texas Hold'em with nine of the other world's best poker players. If you aren't familiar with the Texas Hold'em, you can find a nice summary of the game and its rules [here](http://www.wsop.com/poker-games/texas-holdem/). As a part of her story she mentioned that she first started playing poker when her [brother](https://en.wikipedia.org/wiki/Howard_Lederer), one of the other nine players at the high stakes game, encouraged her to start playing professionally. When she expressed hesitation, he wrote on a small cocktail napkin a list of hands which she should focus on as she began to play. He said, "stick to these hands and you will be fine." In the podcast she didn't reveal what those hands were but this is the kind of problem data science is well ***suited*** (I already hate myself for saying that) to handle.  
![](https://media1.tenor.com/images/4cfa9b4d02ae66bf35d64fe1846e385d/tenor.gif)
  
In fact, Texas Hold'em is ideal for studying many facets of Data Science. Card games are most commonly used as way of discussing the concept of probability, the most fundamental part of statistics. Over the course of this series I will explore a wide range of

## Data

Yep, it's right there in the name. The joke within the data community is that data-science/statistics/economics/whatever-field-that-uses-data is actually only 10% analysis and 90% data cleaning and data management. All the probability knowledge in the world is useless to a data scientist without good data. As they say, "Garbage in, garbage out."

Fortunately, finding good data really isn't a matter of whether it [exists](https://www.livescience.com/54094-how-big-is-the-internet.html) or not. The real question is, do you have the time and/or money to put it in the form you need to perform the analysis you want? As an environment rich with oppotunities of data analysis the topic of poker has an incredibly large amount of data, some of it free and some of it for a price. If you google "[texas holdem data](https://www.google.com/search?q=texas+holdem+data)" the first two links, a StackExchange [post](https://poker.stackexchange.com/questions/881/publicly-available-poker-stats) and a Kaggle [dataset](https://www.kaggle.com/smeilz/poker-holdem-games), are examples of great resources for finding data online. Now, we just need to decide if any of this data helps us answer which hands were on that napkin for Annie's story.

Before we get to more complex analyses, I would like to build a naive understanding of the way Texas Hold'Em works. For the time being I want to know, if you take [betting strategy](http://www.firsttimepokerplayer.com/how-to-play/strategy/betting-guide/) ([game theory](https://math.mit.edu/~apost/courses/18.204_2018/Jingyu_Li_paper.pdf)) out of the equation which hands are more likely to win? In other words, how often do hands win when stacked up against every other hand. Unfortunately, none of the data in the links above are set up for this analysis. For one, I am cheap and don't want to pay for any data but another reason is that many of the free data sets only give data on hands that were revealed at the end of a round. So if someone went all in and everyone else folded the data might not tell us what anyone, even the person who won the pot, had at all. When we turn to the more complex question this won't necessarily be a bad thing but for now we need to figure something else out. At this point there are only two alternatives; We can either turn this post around and go back to discussing probability and [combinatorics](http://mathforum.org/library/drmath/view/65306.html) or we can do something much more data science-y and just make everything up.
![](https://media.giphy.com/media/m7DEwkk9ID7ag/giphy.gif)

## Data Simulation

Yea, I said it. We can just make this stuff up.

Well, ok. We need to make things up in a way that accurately reflects what happens in the real world. This is an extremely useful application of statistics that can help us get at questions even when we have limited data. [Complex](https://www.niss.org/sites/default/files/ruttermigliorettisavarino2009.pdf) problems require much more advanced methods for simulating data but when performed correctly can yield results just as accurate as real data while requiring far less intervention (if live subjects are required) and less money to obtain. Texas Hold'em is a very simple system that affords us the opportunity to see that this does indeed work in principle.

The reason I chose to focus on data and more specifically simulating data for one of my first posts is because it requires the two major skills that make a data scientist a data scientist. Statistics **AND** Programming[^1]. Now is the time where we get to start really using both. The general procedure for simulation is straight foward enough: develop a model of the problem then create a computer program that generates data according to your model. Model development usually requires some initial data in order to calculate distribution parameters which then feed into the code. Coding usually requires careful planning and discipline in order to create an algorithm without errors that can handle your computational restraints. In both areas any number of issues can really stall a data science project. If you can't create an accurate model, nothing will be right. If your code has errors, nothing will be right. Additionally, there are many ways to code an algorithm but while one set of code might work it may take exponentially longer than another set of code to arrive at the same answer. These type of errors are accentuated the larger the dataset you are trying to create.

First, we develop our model. Because Texas Hold'em is a simple system, our model will simply be to recreate the game as it would be played in real life. We start by creating our deck of cards and the table of players we will work with:

```{r CreateGame}
# Define a deck of cards
values <- c("Ace", 2:10, "Jack", "Queen", "King")
suits <- c("Hearts", "Diamonds", "Clubs", "Spades")
full_deck <- expand.grid(values, suits, stringsAsFactors = FALSE)
colnames(full_deck) <- c("Value", "Suit")
full_deck$card_pt <- c(14, 2:10, 11:13)
full_deck$card_cd <- paste(substr(full_deck$Suit, 1, 1), sprintf("%02d", full_deck$card_pt), sep = "")

# Give the players unique IDs
n.players <- 10
round <- data.frame(player_id = 1:n.players)
```

```{r ShowGame, echo = FALSE, results = "asis"}
# Show first five rows
kable(head(full_deck)) %>%
  kable_styling(full_width = F)
```

We only really need the card code going foward but I wanted to show the relationship between what we recognize as a playing card (Ace of Spades, etc.) and what the computer will use (S14, etc.). Now that we have our deck of cards and the players, we must replicate the [dealing](https://www.pagat.com/poker/variants/texasholdem.html) process from start to finish. As described in the link, after shuffling the dealer will burn one card, deal one card to each player and then a second. After the first round of betting the dealer will burn another card before laying three community cards face up in the center of the board - known as the flop. Another round of betting proceeds followed by another burn card and then a fourth community card - the turn. This is also followed by a round of betting, a burn card and the final community card - the river - being dealt. To conclude the game, a last round of betting takes placed followed by any remaining players revealing their cards to determine who won the hand. 

The code below mimics the process I just described for the computer. Instead of shuffling cards by hand, we use a function to randomly select cards as we need them. We assign cards to players and then remove those cards from the deck so they can't be used again. We also remove cards from the deck before dealing any cards out just as in Texas Hold'Em. At the end you can see a table that relates all the dealt cards to specific players. You will notice that the pocket cards are all unique to individual players while the flop, turn and river cards are all the same for everyone. As a side note, storing the community cards in this way isn't necessarily the best method. Anytime you see duplicate information in data, it is a good idea to question whether there is an issue or not. Instead, I could have stored the community cards once and attached them to the game ID which would have saved some computer memory. When looking at larger data, such an inefficiency could really slow down my calculation time. At the same time, I will need to link this information together at some point and doing it now might spare me the need for another calculation down the line. I will need to explore the costs and benefits to determine the best method but will save that for a rainy day. Right now I am not worried about the size of my data because I will be working with relatively small data for this post. Back to the card game, the game has been hypothetically dealt and we can move on to see what hands win the most.

```{r DealGame}
# Burn one card
burn <- sample(nrow(full_deck), 1)
final.deck <- full_deck[-burn, ]

# Deal everyone a single card before going back and giving everyone else a second card
cards_dealt_in <- sample(nrow(final.deck), nrow(round)*2, replace = FALSE)
pocket_cards <- data.frame(matrix(final.deck$card_cd[cards_dealt_in], byrow = FALSE, nrow = nrow(round), ncol = 2))
colnames(pocket_cards) <- c("pocket_card1", "pocket_card2")
round <- cbind(round, pocket_cards)
# Remove dealt cards from deck
final.deck <- final.deck[-cards_dealt_in, ]
  
# Burn one card
burn <- sample(nrow(final.deck), 1)
final.deck <- final.deck[-burn, ]
# The Flop
cards_dealt_in <- sample(nrow(final.deck), 3, replace = FALSE)
flop_cards <- data.frame(matrix(final.deck$card_cd[cards_dealt_in], byrow = TRUE, nrow = nrow(round), ncol = 3))
colnames(flop_cards) <- c("flop_card1", "flop_card2", "flop_card3")
round <- cbind(round, flop_cards)
# Remove dealt cards from deck
final.deck <- final.deck[-cards_dealt_in, ]
  
# Burn one card
burn <- sample(nrow(final.deck), 1)
final.deck <- final.deck[-burn, ]
# The Turn
cards_dealt_in <- sample(nrow(final.deck), 1)
pocket_cards <- data.frame(matrix(final.deck$card_cd[cards_dealt_in], byrow = TRUE, nrow = nrow(round), ncol = 1))
colnames(pocket_cards) <- c("turn_card")
round <- cbind(round, pocket_cards)
# Remove dealt cards from deck
final.deck <- final.deck[-cards_dealt_in, ]

# Burn one card
burn <- sample(nrow(final.deck), 1)
final.deck <- final.deck[-burn, ]
# The River
cards_dealt_in <- sample(nrow(final.deck), 1)
pocket_cards <- data.frame(matrix(final.deck$card_cd[cards_dealt_in], byrow = TRUE, nrow = nrow(round), ncol = 1))
colnames(pocket_cards) <- c("river_card")
round <- cbind(round, pocket_cards)
kable(head(round)) %>%
  kable_styling(full_width = F)
```

Just as I wrote code to simulate a hand of poker, I also have to write code to judge a hand of poker. That is to say, I need to tell the computer how to look at a players 7 cards and pick the 5 that make the strongest combination. That is a rather delicate process with a lot of conditions to worry about, such as the fact that the Ace can make a straight with a 10, Jack, Queen, King or with a 2, 3, 4, 5. However, I'm going to skip walking through that code because it is rather long and I think we are running out of time for this post.
![](https://media.giphy.com/media/3oriNN4R0SDp92UDi8/giphy.gif)

## Results
When examining data, especially data you just made up, it is important to make sure that the data actually makes sense. For the most part data science is about listening to the data and seeing what it has to say for itself, but before you can do that, you need to make sure that the data is telling the truth using information you know to be true before hand. With some data sources, this would entail matching the results of others before doing something unique. In our case, we can compare our results to the theoretical probabilities using our knowledge of statistics and combinatorics.

Talk to you soon.

[^1]: A lot of people only focus on one of these skills. They shouldn't. This will likely be something I bring up again and again.
